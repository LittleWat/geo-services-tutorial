{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-located Clustering and Sequence Mining with Spark, Cassandra and Scala\n",
    "\n",
    "Today, geo-located data is available in a number of domains, ranging from healthcare to financial markets, to social services. In all these domains, extracting patterns and detecting clusters and sequences of events from data has very concrete business outcomes. \n",
    "\n",
    "Cluster analysis on geo-located events is a key step for understanding user's behavior and it is essential for a more personalized customer experience, but also to prevent fraud and cyber attacks. By extracting sequences of events, we can also determine how various venues, products and services are connected, and provide a very detailed analysis about the popularity of certain items, which conversely yields to better user recommendations and richer and more meaningful UI interactions.\n",
    "\n",
    "As more data gets ingested/produced via digital services, itâ€™s key to perform this sort of analytics at scale. In the open source space, technologies such as Spark and Cassandra are definitely instrumental to implement and execute modern data pipelines at scale.\n",
    "\n",
    "\n",
    "#### Synopsis\n",
    "\n",
    "In this Oriole, I will explore with you a number of techniques for understanding users' behavior: We will look at process mining, to understand the _sequence_ of events registered by users. And apply graph analytics to detect the most popular venues in town. Also we will look at how to sketch a geo-fencing, location-based alert service, by applying the DBSCAN clustering algorithm to the geo-located events. \n",
    "\n",
    "We will follow the following steps\n",
    "\n",
    "  - Data extraction from Cassandra\n",
    "  - Process Mining using Spark RDDs\n",
    "  - Graph Analytics with Spark GraphFrames\n",
    "  - Clustering for Geo-Located Data with DBSCAN\n",
    "  \n",
    "If you are interested in a general overview of descriptive analytics, histograms, and pattern detection, please have a look at this companion Oriole notebook \"Anomaly detection and pattern extraction with Spark, Cassandra and Scala\", also available on Safari.\n",
    "\n",
    "\n",
    "#### References and datasets\n",
    "\n",
    "For this analysis, we are going to use the Gowalla Dataset [1]. The Gowalla dataset consists of a table of events, registered by anonymized users. Each event registers a user checking into a geolocated venue at a specific timestamp. The dataset is available at https://snap.stanford.edu/data/loc-gowalla.html\n",
    "\n",
    "A number of venues in this demo have been tagged with an actual name. Thanks to the https://code.google.com/archive/p/locrec/ project (now archived). The project is being developed in the context of the SInteliGIS project financed by the Portuguese Foundation for Science and Technology (FCT) through project grant PTDC/EIA-EIA/109840/2009.\n",
    "\n",
    "[1] E. Cho, S. A. Myers, J. Leskovec. Friendship and Mobility: Friendship and Mobility: User Movement in Location-Based Social Networks ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2011.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "This notebook is running scala code and interfaces to a Spark cluster using the [Apache Toree](https://toree.incubator.apache.org/) project. Furthermore, Spark reads the data from Cassandra tables. Spark interfaces to Cassandra via the [Cassandra-Spark connector](https://github.com/datastax/spark-cassandra-connector). \n",
    "\n",
    "At the time of compiling this notebook, Spark 1.6.1 and Cassandra 3.5 were used. Here below the command to install the Spark - Scala Kernel on Jupiter. More instructions on this topic are available on Apache Toree [website](https://toree.incubator.apache.org/) and [github pages](https://github.com/apache/incubator-toree).\n",
    "\n",
    "This particular Oriole is special in many ways. For instance, as you can see above from the Toree Kernel configuration, next to the ability to connect to Cassandra, it adds (GraphFrames)[https://graphframes.github.io/] as an extra custom Spark package to the mix. \n",
    "\n",
    "```\n",
    "sudo jupyter-toree install --spark_home=${SPARK_HOME} \n",
    "--spark_opts='--packages com.datastax.spark:spark-cassandra\n",
    "-connector_2.10:1.6.0,graphframes:graphframes:0.1.0-spark1.6 \n",
    "--conf spark.cassandra.connection.host=localhost '\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please allow a few seconds for the next cell to complete.  \n",
    "It will start a single-node Spark context on the Oriole container, and connect it to the Oriole scala code cells._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Scala version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Scala Spark Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.188\n"
     ]
    }
   ],
   "source": [
    "val NUM_SAMPLES = 1000\n",
    "val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>\n",
    "  val x = math.random\n",
    "  val y = math.random\n",
    "  x*x + y*y < 1\n",
    "}.count()\n",
    "println(s\"Pi is roughly ${4.0 * count / NUM_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// spark-cassandra connector\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL queries with Spark and Cassandra\n",
    "\n",
    "Cassandra is exposed via a SQL context, so there is not need to learn a separate syntax as Spark will map the query to the available features of the underlying storage system. See below a simple query accessing the name and the id of venues from a cassandra table. Also remember that sql statements are _staged_ but not _executed_ until some actual [actions](http://spark.apache.org/docs/latest/programming-guide.html#actions) needs to be computed. Examples of actions are for instance, **count**(), **first**(), **collect**()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: Venues\n",
    "\n",
    "Let's first have a quick look at the venues. They are stored in the cassandra table `lbsn.venues`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val venues = spark.\n",
    "               read.\n",
    "               format(\"org.apache.spark.sql.cassandra\").\n",
    "               options(Map( \"table\" -> \"venues\", \"keyspace\" -> \"lbsn\" )).\n",
    "               load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by counting the number of venues, and selecting vid `12525`, to get a feeling about how Spark works and getting some facts about the dataset.\n",
    "Feel free to modify the below cells to gain insight on the venue data. Try for instance the `take()` and the `show()` spark dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12525,40.7612551699,-73.977579698,The Museum of Modern Art (MoMA)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.where(\"vid = 12525\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing queries\n",
    "\n",
    "Spark and Cassadra work together when it comes to executing queries. If the query can be executed directly on the database, Spark will offload the query to Cassandra. However, not all queries can be fully performed on cassandra, and that's where the combination Spark-Cassandra gets really handy. For instance, when executing joins, Spark will partition and plan the query _pushing down_ what can be done in Cassandra and perform in Spark the rest of the query. \n",
    "\n",
    "More information can be found on Cassandra Documentation about [using Spark SQL to query data](http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/spark/sparkSqlOverview.html) or on the [Cassandra Spark Connector](https://github.com/datastax/spark-cassandra-connector) pages.\n",
    "\n",
    "#### Joining Cassandra tables with Spark\n",
    "\n",
    "The query here below filters out those events which were registered in the New York City area. As filtering in cassandra cannot by done directly (lat/lon columns which are not indexed in this example), this specific query will first move the data form Cassandra to Spark, and then will perform the filtering in Spark. \n",
    "\n",
    "In general, it's a good practice to push down and filter as much data as early as possible. This practice keeps the throughput low and minimize the data transfered from one system to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: Events\n",
    "\n",
    "Let's first have a quick look at the events. They are stored in the cassandra table `lbsn.events`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP VIEW IF EXISTS events\")\n",
    "\n",
    "val createDDL = \"\"\"\n",
    "     CREATE TEMPORARY VIEW events\n",
    "     USING org.apache.spark.sql.cassandra\n",
    "     OPTIONS (\n",
    "     table \"events\",\n",
    "     keyspace \"lbsn\",\n",
    "     pushdown \"true\")\"\"\"\n",
    "\n",
    "// Creates Catalog Entry registering an existing Cassandra Table\n",
    "spark.sql(createDDL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val events   = spark.sql(\"\"\"select ts, uid, lat, lon, vid from events where\n",
    "                            lon>-74.2589 and lon<-73.7004 and \n",
    "                            lat> 40.4774 and lat< 40.9176\n",
    "                      \"\"\").as(\"events\").orderBy(\"uid\", \"ts\").repartition(8, $\"uid\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- vid: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here above, Spark dataframe can extract the schema from the Cassandra/Spark SQL query. In this particular case, they mapping of cassandra types to Spark Dataframe types is performed by the [Cassandra-Spark connector](https://github.com/datastax/spark-cassandra-connector). Each event consists in a timestamp, the user and venue id and the location of the event (latitude and longitude).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the events data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at some recorded events. A quick way to do so is with the `show()` Dataframe spark method.\n",
    "Feel free to modify the below cells to gain insight on the venue data. Try for instance the `take()`, `show()`, `count()` spark dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-------------+------------------+------+\n",
      "|                  ts|uid|          lat|               lon|   vid|\n",
      "+--------------------+---+-------------+------------------+------+\n",
      "|2010-04-18 21:23:...| 88| 40.645919692|    -73.7779355049| 17417|\n",
      "|2010-04-18 22:33:...| 88|40.7628367545|    -73.9825987816|150676|\n",
      "|2010-04-18 23:38:...| 88|    40.763479|-73.97908000000002|972311|\n",
      "|2010-04-19 12:04:...| 88|40.7417466987|     -73.993421425|105068|\n",
      "|2010-04-19 13:53:...| 88|40.7422661063|-73.98375749590001|851661|\n",
      "+--------------------+---+-------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into anomaly detection of geo-located data, let's perform some more basic queries.  \n",
    "Herebelow, it is shown how to count events registered by user `uid=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// User 0: how many check-ins?\n",
    "\n",
    "events.where(\"uid=0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process mining with Spark RDDs\n",
    "\n",
    "The first step, in order to do process mining, is to collect sequences of events. In particular, the following code will take chronologically consecutive events and bundle them in pairs for a specific user. These pairs consists of two venue ids, namely source and destination, defining where each user is coming from, going to respectively. \n",
    "\n",
    "The steps in the following code are:\n",
    "\n",
    "  - Convert the DataFrame to an RDD\n",
    "  - Select uid as the key for the PairRDD\n",
    "  - Reshape the PairRDD from \"tall\" to \"wide\"\n",
    "  - Sort chronologically all the checked-in venues for each user\n",
    "  - Extract pairs from each sequence of checked-in venues per user\n",
    "  - Reshape the PairRDD from \"wide\" to \"tall\" again\n",
    "  - Convert back the PairRDD to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// process mining\n",
    "val g_df = events.\n",
    "  select($\"ts\", $\"uid\", $\"vid\").\n",
    "  rdd.\n",
    "  map(row => (row.getLong(1), List( (row.getTimestamp(0), row.getLong(2)) ))).\n",
    "  reduceByKey(_ ++ _).\n",
    "  mapValues( x =>\n",
    "    x.sortWith(_._1.getTime < _._1.getTime).\n",
    "      map(_._2)\n",
    "  ).\n",
    "  mapValues(_.sliding(2).toList).\n",
    "  flatMap(_._2).\n",
    "  map(\n",
    "    _ match {\n",
    "      case List(a, b) => Some((a, b))\n",
    "      case _ => None\n",
    "  }).\n",
    "  flatMap(x => x).\n",
    "  toDF(\"src\", \"dst\").\n",
    "  repartition(8,$\"src\").\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This newly created DataFrame is used to create a graph, where the nodes are the venues and the edges are connections of users checking-in from the one venue to the next. Let's do a bit of pruning on this graph by removing self-referencing nodes, and by filterning out nodes which are not connected at all, as shown below in the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   src|   dst|\n",
      "+------+------+\n",
      "|620588|636683|\n",
      "|226768|664731|\n",
      "|664731|101538|\n",
      "|167249|874897|\n",
      "|226768|176668|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val edges_df = g_df.\n",
    "  groupBy($\"src\",$\"dst\").\n",
    "  count().\n",
    "  select($\"src\",$\"dst\").\n",
    "  filter($\"src\" !== $\"dst\")\n",
    "\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|     id|\n",
      "+-------+\n",
      "|1046872|\n",
      "| 578977|\n",
      "| 216302|\n",
      "|  11745|\n",
      "| 740603|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val nodes_df = edges_df.\n",
    "  select($\"src\").\n",
    "  unionAll(edges_df.select($\"dst\")).\n",
    "  distinct().\n",
    "  toDF(\"id\")\n",
    "\n",
    "nodes_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venues graph\n",
    "\n",
    "Congratulation! You have just constructed a graph, where venues are connected to other venues according to sequence of events recorded by users. Let's have a quick look at the size of this graph. By the way for those of you new to scala, you can embedd code in a scala string using the `${ ... }` construct as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "# nodes (venues)      = 17220\n",
      "# edges (connections) = 87279\n"
     ]
    }
   ],
   "source": [
    "println(s\"# nodes (venues)      = ${nodes_df.count()}\")\n",
    "println(s\"# edges (connections) = ${edges_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert two Spark DataFrames, one for the nodes and the other for the edges as a GraphFrame g, as shown here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.graphframes.GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val v = nodes_df.orderBy(\"id\")\n",
    "val e = edges_df.orderBy(\"src\", \"dst\")\n",
    "\n",
    "val g = GraphFrame(nodes_df, edges_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph: Node degree analysis\n",
    "\n",
    "Let's start a basic graph analysis looking first at in-degree. The in-degree is the property which counts the number of incoming edges for each node of the graph. We can use in-degree to sort which venues are receiving most incoming connections from other venues. This is one of the easiest ways to see which venues are popular. Let's have a look at which nodes/venues in the graph have at least 50 incoming connections, and let's display this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.desc\n",
    "val vertices_indeg = g.inDegrees.filter($\"inDegree\">50).sort(desc(\"inDegree\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Top in-degree nodes: 111\n",
      "+------+--------+---------------------------------+\n",
      "|vid   |inDegree|name                             |\n",
      "+------+--------+---------------------------------+\n",
      "|12505 |657     |LGA LaGuardia Airport            |\n",
      "|23261 |600     |JFK John F. Kennedy International|\n",
      "|11844 |580     |Times Square                     |\n",
      "|13022 |442     |Grand Central Terminal           |\n",
      "|24963 |352     |EWR Newark Liberty International |\n",
      "|11875 |278     |Madison Square Garden            |\n",
      "|12525 |273     |The Museum of Modern Art (MoMA)  |\n",
      "|106840|247     |Union Square                     |\n",
      "|11834 |234     |Bryant Park                      |\n",
      "|11720 |232     |Yankee Stadium                   |\n",
      "+------+--------+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val popular_venues = vertices_indeg.\n",
    "    join(venues, vertices_indeg(\"id\") === venues(\"vid\"), \"inner\").\n",
    "    sort($\"inDegree\".desc).\n",
    "    select(\"vid\", \"inDegree\", \"name\")\n",
    "    \n",
    "println(s\"Top in-degree nodes: ${ popular_venues.count()}\")\n",
    "popular_venues.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most transportation hubs would probably will be the most pospular according to this metric. But what about other way of measuring the popularity of a venue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph filtering\n",
    "\n",
    "Before moving on to page rank let's make the graph smaller. For the sake of performance and fast execution of this next cells, we are limiting the analysis to only those venues with at least 50 incoming connections (in-degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph: vertices=111 edges=2027\n"
     ]
    }
   ],
   "source": [
    "val vertices  = vertices_indeg.select(\"id\")\n",
    "\n",
    "val edges_src = edges_df.join(vertices, vertices(\"id\")  === edges_df(\"src\"), \"inner\").select(\"src\", \"dst\")\n",
    "val edges     = edges_src.join(vertices, vertices(\"id\") === edges_src(\"dst\"), \"inner\").select(\"src\", \"dst\")\n",
    "\n",
    "println(s\"graph: vertices=${vertices.count()} edges=${edges.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph and Page Rank\n",
    "\n",
    "The above node and edge DataFrames describes how users are moving from venue to venue. We can now calculate which venues attract more users. This can be done using the page rank algorithm. The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly walking in the city will arrive at any particular venue. This analysis can be executed in Spark using [GraphFrames](http://graphframes.github.io/). GraphFrames is a package for Apache Spark which provides DataFrame-based graph analytics, including the PageRank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val v = vertices.orderBy(\"id\").repartition(8, $\"id\").cache()\n",
    "val e = edges.orderBy(\"src\", \"dst\").repartition(8, $\"src\", $\"dst\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val g = GraphFrame(v, e)\n",
    "val results = g.pageRank.resetProbability(0.05).maxIter(10).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------------------------+\n",
      "|vid   |pagerank          |name                             |\n",
      "+------+------------------+---------------------------------+\n",
      "|12505 |47.025529046618324|LGA LaGuardia Airport            |\n",
      "|23261 |43.786106629521555|JFK John F. Kennedy International|\n",
      "|11844 |38.66027839728225 |Times Square                     |\n",
      "|13022 |30.332699976352583|Grand Central Terminal           |\n",
      "|24963 |27.040978475705277|EWR Newark Liberty International |\n",
      "|11875 |19.71505524245435 |Madison Square Garden            |\n",
      "|12525 |18.03550609957437 |The Museum of Modern Art (MoMA)  |\n",
      "|11720 |17.15123666057204 |Yankee Stadium                   |\n",
      "|106840|17.08807261065496 |Union Square                     |\n",
      "|11834 |16.021149068906222|Bryant Park                      |\n",
      "|12313 |14.530252724392344|Empire State Building            |\n",
      "|17710 |12.700090614679826|Madison Square Park              |\n",
      "|14151 |12.570035992405163|Rockefeller Center               |\n",
      "+------+------------------+---------------------------------+\n",
      "only showing top 13 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val result_pr = results.vertices.select(\"id\", \"pagerank\")\n",
    "val popular_venues = result_pr.\n",
    "    join(venues, result_pr(\"id\") === venues(\"vid\"), \"inner\").\n",
    "    select(\"vid\", \"pagerank\", \"name\")\n",
    "\n",
    "popular_venues.sort($\"pagerank\".desc).show(13, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this algorithm provides a \"popularity\" factor for each checked-in venue. This feature can be used to further discriminate anomalies based on the rank of the venue, for instance combining it with the probability of checking in a specific time of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-Location: density based\n",
    "\n",
    "We will now cluster events based on the [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN). DBSCAN is clustering events depending on the density of the events provided. Since the clusters emerge locally by looking for neighboring points, clusters of various shapes can be detected. Points that are isolated and too far from any other point are assigned to a special cluster of outliers. These discerning properties make the DBSCAN algorithm a good candidate for clustering geolocated events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data by transforming the events DataFrame, into a PairRDD. In particular, for geolocated data, we choose the key to be the user identifier, and the value to be the aggregated list of all check-ins posted by that given user. The geolocated data is arranged in a n-by-2 matrix, where the first column represents the latitude and the second column the longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| uid|count|\n",
      "+----+-----+\n",
      "| 578| 1641|\n",
      "|  22| 1290|\n",
      "| 842| 1145|\n",
      "|4985| 1063|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val top_users = events.\n",
    "    groupBy($\"uid\").\n",
    "    count().\n",
    "    filter($\"count\" > 1000).\n",
    "    alias(\"top_users\")\n",
    "    \n",
    "top_users.sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+------------------+------+----+-----+\n",
      "|ts                   |lat          |lon               |vid   |uid |count|\n",
      "+---------------------+-------------+------------------+------+----+-----+\n",
      "|2009-12-30 20:23:39.0|40.7663583   |-73.9833973667    |218064|4985|1063 |\n",
      "|2009-12-30 20:23:53.0|40.7665252686|-73.9829705584    |37819 |4985|1063 |\n",
      "|2009-12-30 20:24:02.0|40.76665848  |-73.98390340000002|269570|4985|1063 |\n",
      "|2009-12-30 20:24:26.0|40.7671166667|-73.9823264       |127312|4985|1063 |\n",
      "|2009-12-30 20:24:35.0|40.7673913454|-73.9814347029    |110749|4985|1063 |\n",
      "+---------------------+-------------+------------------+------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val top_events = events.\n",
    "    join(top_users, top_users(\"uid\")  === events(\"uid\"), \"inner\").\n",
    "    drop(events(\"uid\"))\n",
    "\n",
    "top_events.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From long to wide\n",
    "\n",
    "What would like to do now, as a preparation for the DBSCAN clustering algorithm is to take all events registered by each user and store them as a single array of tuples. One way to do that is to create a key-value RDD (check http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs), where the key is the user id and the value is an array of co-ordinates tuples. How to go wide? we concatenate vectors using the `reduceByKey` rdd function, in this way we can ridistributed the data in a format which works well for our DBSCAN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import breeze.linalg._\n",
    "import breeze.linalg.DenseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val e_df = top_events.\n",
    "  select(\"uid\",\"lat\",\"lon\").\n",
    "  rdd.map(row => (row.getLong(0), Array(row.getDouble(1), row.getDouble(2))) ).\n",
    "  reduceByKey( _ ++ _).\n",
    "  mapValues(v => new DenseMatrix(v.length/2,2,v, 0, 2, true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just created is a complex data structure, for debug and visualization, let's create a well formatted printing utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatUserEvents(x: Tuple2[Long, DenseMatrix[Double]]) : Unit = {\n",
    "    val arr = x._2\n",
    "    val n = math.min( 5 , arr.rows) - 1\n",
    "    val slice = arr(0 to n, ::)\n",
    "    println(s\"uid = ${x._1}\")\n",
    "    println(s\"events count = ${arr.rows}\")\n",
    "    println(\"lat,lon = \")\n",
    "    println(slice)\n",
    "    if (arr.rows > 5) println(s\"... ${arr.rows- 5} more rows\")\n",
    "    println(\"-\"*30)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the formatting function, with spark and scala foreach statements.  \n",
    "See below a formatted output describing the events related to three users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid = 4985\n",
      "events count = 1063\n",
      "lat,lon = \n",
      "40.7663583     -73.9833973667      \n",
      "40.7665252686  -73.9829705584      \n",
      "40.76665848    -73.98390340000002  \n",
      "40.7671166667  -73.9823264         \n",
      "40.7673913454  -73.9814347029      \n",
      "... 1058 more rows\n",
      "------------------------------\n",
      "uid = 578\n",
      "events count = 1641\n",
      "lat,lon = \n",
      "40.7444201864  -73.98721218109999  \n",
      "40.7457482667  -73.9850020333      \n",
      "40.7443825333  -73.9783781667      \n",
      "40.7428595014  -73.9768588543      \n",
      "40.7428595014  -73.9768588543      \n",
      "... 1636 more rows\n",
      "------------------------------\n",
      "uid = 842\n",
      "events count = 1145\n",
      "lat,lon = \n",
      "40.7164383   -74.0125343         \n",
      "40.71685842  -74.01399805        \n",
      "40.71575957  -74.01116800000001  \n",
      "40.717054    -74.013316          \n",
      "40.71685483  -74.01230673        \n",
      "... 1140 more rows\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "e_df.take(3).foreach(e => formatUserEvents(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now cluster the events for each user according to the DBSCAN algorithm. This algorithm with cluster those user's events in groups. The rest of the code below reduces those groups to bounding boxes. Next we will use the extracted bounding boxes to score events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import breeze.numerics._\n",
    "import breeze.linalg._\n",
    "\n",
    "def euclideanDistance (a: DenseVector[Double], b: DenseVector[Double]) = norm(a-b, 2)\n",
    "\n",
    "// 1deg at 40deg latitude is 111034.61 meters\n",
    "// set radius at about 200 mt (0.002 * 111034.61)\n",
    "// which is 0.002 in decimal degrees https://en.wikipedia.org/wiki/Decimal_degrees\n",
    "\n",
    "val eps = 0.002\n",
    "val min_points = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nak.cluster._\n",
    "import nak.cluster.GDBSCAN._\n",
    "\n",
    "def dbscan(v : breeze.linalg.DenseMatrix[Double]) = {\n",
    "\n",
    "  val gdbscan = new GDBSCAN(\n",
    "    DBSCAN.getNeighbours(eps, distance=euclideanDistance),\n",
    "    DBSCAN.isCorePoint(min_points)\n",
    "  )\n",
    "\n",
    "  // core DBSCAN algorithm\n",
    "  val clusters = gdbscan cluster v\n",
    "  \n",
    "  // reducing the clusters to bounding boxes\n",
    "  // for simplicity: each user could \n",
    "  clusters.map(\n",
    "    cluster => (\n",
    "      cluster.id.toInt, \n",
    "      cluster.points.size, \n",
    "      cluster.points.map(_.value(0)).min,\n",
    "      cluster.points.map(_.value(1)).min,\n",
    "      cluster.points.map(_.value(0)).max,\n",
    "      cluster.points.map(_.value(1)).max\n",
    "    )\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next is probably the most powerful one-liner in this tutorial.   \n",
    "It will stage the DBSCAN clustering algorithm for all users and their respective events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bboxRdd = e_df.mapValues(dbscan(_)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert back the RDDs to a DataFrame. Now we have a table describing clusters. Each row defines a cluster in terms of user id, cluster id, the number of cluster's events and the bounding box of the cluster. Each user can have multiple clusters, and some users might have no cluster at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+------------------+------------------+------------------+--------------+\n",
      "| uid|cid|csize|           lat_min|           lon_min|           lat_max|       lon_max|\n",
      "+----+---+-----+------------------+------------------+------------------+--------------+\n",
      "|4985|  4|   14|     40.7665739833|    -73.9829850197|40.769262733299996|  -73.98122905|\n",
      "|4985| 10|   25|       40.76349341|      -73.98234528|     40.7652683667|-73.9789733887|\n",
      "|4985| 15|    6|        40.7631676|       -73.9792362|     40.7649154663|-73.9781703833|\n",
      "|4985| 24|   12|     40.7626658695|    -73.9765036134|     40.7647866916|-73.9739406109|\n",
      "|4985| 29|    7|     40.7604105667|      -73.97418069|40.762592967399996|  -73.97242375|\n",
      "|4985| 38|   19|     40.7620566327|    -73.9716011167|40.764846633299996|-73.9698984337|\n",
      "|4985| 40|   35|40.759771316700004|    -73.9695874833|       40.76248225|  -73.96590763|\n",
      "|4985| 44|   21|       40.75770225|-73.96918773649999|40.760391691500004|-73.9659552814|\n",
      "|4985| 45|   12|     40.7608603333|     -73.966573727|     40.7644279489|     -73.96472|\n",
      "|4985| 46|    4|       40.76836514|      -73.96224765|     40.7700063885|-73.9603936294|\n",
      "+----+---+-----+------------------+------------------+------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val bbox_df = bboxRdd.\n",
    "  flatMapValues(x => x).\n",
    "  map(x => (x._1, x._2._1, x._2._2,x._2._3,x._2._4,x._2._5,x._2._6)).\n",
    "  toDF(\"uid\", \"cid\", \"csize\", \"lat_min\", \"lon_min\", \"lat_max\", \"lon_max\").\n",
    "  filter($\"cid\" > 0)\n",
    "\n",
    "bbox_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Events: looking for outliers\n",
    "\n",
    "We will now score events, and look if some of them are located outside the computed clusters' bounding boxes. Firstly, we join the table of events with the table of clusters. Let's filter out users which do not have enough points as those users have no clusters associated with them and there is no sufficient data to determine outliers. In the code above, we need a user to have at least 3 events in a region of 0.1 degrees in order to have a DBSCAN cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----+-------------+------------------+-------------+--------------+------------------+------------+\n",
      "|ts                   |uid |lat          |lon               |lat_min      |lon_min       |lat_max           |lon_max     |\n",
      "+---------------------+----+-------------+------------------+-------------+--------------+------------------+------------+\n",
      "|2010-10-16 04:29:09.0|4985|40.7727768667|-73.95842066670002|40.7665739833|-73.9829850197|40.769262733299996|-73.98122905|\n",
      "|2010-10-16 04:28:54.0|4985|40.7726443798|-73.9579036832    |40.7665739833|-73.9829850197|40.769262733299996|-73.98122905|\n",
      "|2010-10-16 04:28:37.0|4985|40.776251689 |-73.955940897     |40.7665739833|-73.9829850197|40.769262733299996|-73.98122905|\n",
      "|2010-10-16 04:28:28.0|4985|40.77383416  |-73.95962762      |40.7665739833|-73.9829850197|40.769262733299996|-73.98122905|\n",
      "|2010-10-16 04:28:19.0|4985|40.7728809121|-73.9556687823    |40.7665739833|-73.9829850197|40.769262733299996|-73.98122905|\n",
      "+---------------------+----+-------------+------------------+-------------+--------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val bbox_events = events.\n",
    "  join(bbox_df, Seq(\"uid\"), \"full\").\n",
    "  filter($\"lat_min\".isNotNull).\n",
    "  filter($\"lat_max\".isNotNull).\n",
    "  filter($\"lon_min\".isNotNull).\n",
    "  filter($\"lon_max\".isNotNull).\n",
    "  select($\"events.ts\",$\"uid\",$\"lat\",$\"lon\",$\"lat_min\",$\"lon_min\",$\"lat_max\",$\"lon_max\")\n",
    "\n",
    "bbox_events.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the newer Dataset API, which is a DataFrame where row are handled as typed objects. In particular, we are converting the events row into a `EventDetected` object and then we check if the event is within the boundary of the given cluster. Since each user might have more than one cluster, we check each event against all the user's clusters and we consider it an outlier if none of the check is returns a positive outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+------------------+------+\n",
      "|                  ts| uid|          lat|               lon|detect|\n",
      "+--------------------+----+-------------+------------------+------+\n",
      "|2010-10-16 04:29:...|4985|40.7727768667|-73.95842066670002| false|\n",
      "|2010-10-16 04:28:...|4985|40.7726443798|    -73.9579036832| false|\n",
      "|2010-10-16 04:28:...|4985| 40.776251689|     -73.955940897| false|\n",
      "|2010-10-16 04:28:...|4985|  40.77383416|      -73.95962762| false|\n",
      "|2010-10-16 04:28:...|4985|40.7728809121|    -73.9556687823| false|\n",
      "+--------------------+----+-------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val scored_events = bbox_events.\n",
    "                      withColumn(\"detect\", \n",
    "                                 $\"lon\" >= $\"lon_min\" &&\n",
    "                                 $\"lon\" <= $\"lon_max\" &&\n",
    "                                 $\"lat\" >= $\"lat_min\" &&\n",
    "                                 $\"lat\" <= $\"lat_max\").\n",
    "                      select($\"ts\",$\"uid\",$\"lat\",$\"lon\", $\"detect\")\n",
    "scored_events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below the outlier scoring for `uid=22`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------------+------------------+------+\n",
      "|                  ts|uid|               lat|               lon|detect|\n",
      "+--------------------+---+------------------+------------------+------+\n",
      "|2010-10-17 21:43:...| 22|40.755876281599996|    -73.9823949337| false|\n",
      "|2010-10-14 00:55:...| 22|40.748029583800005|    -74.0071158496| false|\n",
      "|2010-10-13 04:49:...| 22|     40.7258684255|     -73.983424902| false|\n",
      "|2010-10-12 23:58:...| 22|     40.7277736384|     -73.985715066| false|\n",
      "|2010-10-12 04:23:...| 22|       40.76773025|    -73.8994259833| false|\n",
      "|2010-10-12 02:45:...| 22|     40.6438845363|-73.78280639649999| false|\n",
      "|2010-10-06 11:31:...| 22|     40.6438845363|-73.78280639649999| false|\n",
      "|2010-09-21 12:47:...| 22|40.723510469299995|    -73.9884352684| false|\n",
      "|2010-09-17 23:57:...| 22|     40.6438845363|-73.78280639649999| false|\n",
      "|2010-09-17 00:43:...| 22|40.748029583800005|    -74.0071158496| false|\n",
      "|2010-09-14 01:57:...| 22|     40.7331512078|    -74.0058752721| false|\n",
      "|2010-09-14 01:21:...| 22|     40.7334206534|    -74.0044514831| false|\n",
      "|2010-09-12 00:33:...| 22|     40.7304645196|    -74.0021956229| false|\n",
      "|2010-09-10 21:32:...| 22|     40.7600881033|     -73.973275284| false|\n",
      "|2010-09-06 15:01:...| 22|     40.7657052487|    -73.9472579956|  true|\n",
      "|2010-09-02 21:33:...| 22|     40.7657052487|    -73.9472579956|  true|\n",
      "|2010-09-02 02:54:...| 22|     40.7589340167|    -73.9684581667| false|\n",
      "|2010-09-01 21:03:...| 22|     40.7667465495|    -73.9537504797| false|\n",
      "|2010-09-01 21:02:...| 22|     40.7667465495|    -73.9537504797| false|\n",
      "|2010-08-30 23:54:...| 22|     40.7657052487|    -73.9472579956|  true|\n",
      "+--------------------+---+------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_events.\n",
    "  filter(\"uid == 22\").\n",
    "  show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see three items are found outside those bounding boxes. Although this is not yet a strong indicator for an anomaly per se, it can constitute a very relevant signal if combined with other signals as seen above. Many improvements can be done to the above core idea, for instance, by including relations and interaction between users and more refined analysis of clusters, using for instance convex hulls instead of bounding boxes and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed this notebook, thanks for keeping up with me till here. Best wishes for your data science projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
