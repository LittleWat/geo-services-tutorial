{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection and pattern extraction with Spark, Cassandra and Scala\n",
    "\n",
    "Today, geo-located data is available in a number of domains, ranging from healthcare to financial markets, to social services. In all these domains, extracting patterns and detecting anomalies and novelties from data has very concrete business outcomes. \n",
    "\n",
    "Anomaly detection can be defined as the process of finding which samples in the given dataset do not follow the given patterns and behave as though they were produced by a different mechanism. From detection follows action. Depending on the domain and the use case, we define them as anomalies or novelties and these signals are the triggers for applications such as personalized marketing and fraud alerting and notification.\n",
    "\n",
    "As more data gets ingested/produced via digital services, itâ€™s key to perform this sort of analytics at scale. In the open source space, technologies such as Spark and Cassandra are definitely instrumental to implement and execute modern data pipelines at scale.\n",
    "\n",
    "#### Synopsis\n",
    "\n",
    "In this Oriole, we will collect data from Cassandra and bring it up to Spark for further analysis. We will see how to prepare and analyze the data using a combination of Spark and Cassandra exploratory queries. Then we will perform the following data analyses:\n",
    "\n",
    "  - Descriptive statistics \n",
    "  - Events histograms\n",
    "  - Extract users' preferences\n",
    "  - Detect novel/anomalous events\n",
    "\n",
    "#### References and datasets\n",
    "\n",
    "For this analysis, we are going to use the Gowalla Dataset [1]. The Gowalla dataset consists of a table of events, registered by anonymized users. Each event registers a user checking into a geolocated venue at a specific timestamp. The dataset is available at https://snap.stanford.edu/data/loc-gowalla.html\n",
    "\n",
    "A number of venues in this demo have been tagged with an actual name. Thanks to the https://code.google.com/archive/p/locrec/ project (now archived). The project is being developed in the context of the SInteliGIS project financed by the Portuguese Foundation for Science and Technology (FCT) through project grant PTDC/EIA-EIA/109840/2009.\n",
    "\n",
    "[1] E. Cho, S. A. Myers, J. Leskovec. Friendship and Mobility: Friendship and Mobility: User Movement in Location-Based Social Networks ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2011.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "This notebook is running scala code and interfaces to a Spark cluster using the [Apache Toree](https://toree.incubator.apache.org/) project. Furthermore, Spark reads the data from Cassandra tables. Spark interfaces to Cassandra via the [Cassandra-Spark connector](https://github.com/datastax/spark-cassandra-connector). \n",
    "\n",
    "At the time of compiling this notebook, Spark 1.6.1 and Cassandra 3.5 were used. Here below the command to install the Spark - Scala Kernel on Jupiter. More instructions on this topic are available on Apache Toree [website](https://toree.incubator.apache.org/) and [github pages](https://github.com/apache/incubator-toree).\n",
    "\n",
    "```\n",
    "sudo jupyter-toree install --spark_home=${SPARK_HOME} \\\n",
    "--spark_opts='--packages com.datastax.spark:spark-cassandra-connector_2.10:1.6.0 --conf spark.cassandra.connection.host=localhost'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please allow a few seconds for the next cell to complete.  \n",
    "It will start a single-node Spark context on the Oriole container, and connect it to the Oriole scala code cells._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Scala version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Scala Spark Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.143784\n"
     ]
    }
   ],
   "source": [
    "val NUM_SAMPLES = 1000000\n",
    "val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>\n",
    "  val x = math.random\n",
    "  val y = math.random\n",
    "  x*x + y*y < 1\n",
    "}.count()\n",
    "println(s\"Pi is roughly ${4.0 * count / NUM_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// spark-cassandra connector\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL queries with Spark and Cassandra\n",
    "\n",
    "Cassandra is exposed via a SQL context, so there is not need to learn a separate syntax as Spark will map the query to the available features of the underlying storage system. See below a simple query accessing the name and the id of venues from a cassandra table. Also remember that sql statements are _staged_ but not _executed_ until some actual [actions](http://spark.apache.org/docs/latest/programming-guide.html#actions) needs to be computed. Examples of actions are for instance, **count**(), **first**(), **collect**()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: Venues\n",
    "\n",
    "Let's first have a quick look at the venues. They are stored in the cassandra table `lbsn.venues`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val venues = spark.\n",
    "               read.\n",
    "               format(\"org.apache.spark.sql.cassandra\").\n",
    "               options(Map( \"table\" -> \"venues\", \"keyspace\" -> \"lbsn\" )).\n",
    "               load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by counting the number of venues, and selecting vid `12525`, to get a feeling about how Spark works and getting some facts about the dataset.\n",
    "Feel free to modify the below cells to gain insight on the venue data. Try for instance the `take()` and the `show()` spark dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12525,40.7612551699,-73.977579698,The Museum of Modern Art (MoMA)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.where(\"vid = 12525\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing queries\n",
    "\n",
    "Spark and Cassadra work together when it comes to executing queries. If the query can be executed directly on the database, Spark will offload the query to Cassandra. However, not all queries can be fully performed on cassandra, and that's where the combination Spark-Cassandra gets really handy. For instance, when executing joins, Spark will partition and plan the query _pushing down_ what can be done in Cassandra and perform in Spark the rest of the query. \n",
    "\n",
    "More information can be found on Cassandra Documentation about [using Spark SQL to query data](http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/spark/sparkSqlOverview.html) or on the [Cassandra Spark Connector](https://github.com/datastax/spark-cassandra-connector) pages.\n",
    "\n",
    "#### Joining Cassandra tables with Spark\n",
    "\n",
    "The query here below filters out those events which were registered in the New York City area. As filtering in cassandra cannot by done directly (lat/lon columns which are not indexed in this example), this specific query will first move the data form Cassandra to Spark, and then will perform the filtering in Spark. \n",
    "\n",
    "In general, it's a good practice to push down and filter as much data as early as possible. This practice keeps the throughput low and minimize the data transfered from one system to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: Events\n",
    "\n",
    "Let's first have a quick look at the events. They are stored in the cassandra table `lbsn.events`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP VIEW IF EXISTS events\")\n",
    "\n",
    "val createDDL = \"\"\"\n",
    "     CREATE TEMPORARY VIEW events\n",
    "     USING org.apache.spark.sql.cassandra\n",
    "     OPTIONS (\n",
    "     table \"events\",\n",
    "     keyspace \"lbsn\",\n",
    "     pushdown \"true\")\"\"\"\n",
    "\n",
    "// Creates Catalog Entry registering an existing Cassandra Table\n",
    "spark.sql(createDDL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val events   = spark.sql(\"\"\"select ts, uid, lat, lon, vid from events where\n",
    "                            lon>-74.2589 and lon<-73.7004 and \n",
    "                            lat> 40.4774 and lat< 40.9176\n",
    "                      \"\"\").as(\"events\").orderBy(\"uid\", \"ts\").repartition(8, $\"uid\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- vid: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here above, Spark dataframe can extract the schema from the Cassandra/Spark SQL query. In this particular case, they mapping of cassandra types to Spark Dataframe types is performed by the [Cassandra-Spark connector](https://github.com/datastax/spark-cassandra-connector). Each event consists in a timestamp, the user and venue id and the location of the event (latitude and longitude).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the events data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at some recorded events. A quick way to do so is with the `show()` Dataframe spark method.\n",
    "Feel free to modify the below cells to gain insight on the venue data. Try for instance the `take()`, `show()`, `count()` spark dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-------------+------------------+------+\n",
      "|                  ts|uid|          lat|               lon|   vid|\n",
      "+--------------------+---+-------------+------------------+------+\n",
      "|2010-04-18 21:23:...| 88| 40.645919692|    -73.7779355049| 17417|\n",
      "|2010-04-18 22:33:...| 88|40.7628367545|    -73.9825987816|150676|\n",
      "|2010-04-18 23:38:...| 88|    40.763479|-73.97908000000002|972311|\n",
      "|2010-04-19 12:04:...| 88|40.7417466987|     -73.993421425|105068|\n",
      "|2010-04-19 13:53:...| 88|40.7422661063|-73.98375749590001|851661|\n",
      "+--------------------+---+-------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into anomaly detection of geo-located data, let's perform some more basic queries.  \n",
    "Herebelow, it is shown how to count events registered by user `uid=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// User 0: how many check-ins?\n",
    "\n",
    "events.where(\"uid=0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Cassandra tables in Spark.\n",
    "\n",
    "One of the advantages of connecting Cassandra and Spark, is the fact that you can now merge and join Cassandra tables. You can actually keep coding with Spark DataFrames, as the join will happen in the background and the necessary queries will be pushed back from Spark to Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val df_ny = events.\n",
    "  join(venues, events(\"vid\") === venues(\"vid\"), \"inner\").\n",
    "  select(\"ts\", \"uid\", \"events.lat\", \"events.lon\", \"events.vid\",\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spark DataFrame df_ny above is going to be the starting point for our data analysis.     Each row records the event's timestamp, the user id, the geo-location (latitude and longitude) of the event and finally the venue's id and venue's name.  \n",
    "\n",
    "Here below let's have a look at 5 events checkd in by `uid=0`, in no particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---+-------------+------------------+-----+---------------------------------+\n",
      "|ts                   |uid|lat          |lon               |vid  |name                             |\n",
      "+---------------------+---+-------------+------------------+-----+---------------------------------+\n",
      "|2010-10-07 15:27:40.0|0  |40.6438845363|-73.78280639649999|23261|JFK John F. Kennedy International|\n",
      "|2010-10-07 20:14:44.0|0  |40.7515076167|-73.9755          |34484|Chrysler Building                |\n",
      "|2010-10-07 20:31:48.0|0  |40.7484436586|-73.9857316017    |12313|Empire State Building            |\n",
      "|2010-10-07 21:02:01.0|0  |40.7458101407|-73.98822069170002|60450|Ace Hotel                        |\n",
      "|2010-10-07 21:58:31.0|0  |40.7422010764|-73.9879953861    |17710|Madison Square Park              |\n",
      "+---------------------+---+-------------+------------------+-----+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ny.filter($\"uid\"===0).show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing SQL with custom defined functions.\n",
    "\n",
    "Spark dataframes can also be filtered and transformed programmatically via a number of [pre-defined functions](https://spark.apache.org/docs/1.6.1/api/scala/#org.apache.spark.sql.functions$), such as min, sum, stddev, and many more. Some of those are shown in the next code sections. \n",
    "\n",
    "Next to the default set of pre-defined dataframe and column functions, it is possible to define the user-defined-functions (udf's). In the code below, we will create two UDF's to transform the timestamp to the day of the week and the hour of the day values, computed according to a given local timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// UDF functions for SQL-like operations on columns\n",
    "import org.joda.time.DateTime\n",
    "import org.joda.time.DateTimeZone\n",
    "\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val  dayofweek = udf( (ts: Timestamp, tz: String) => {\n",
    "  val dt = new DateTime(ts,DateTimeZone.forID(tz))\n",
    "  // Monday starts at 1, but we would like to count from zero\n",
    "  dt.getDayOfWeek() -1\n",
    "})\n",
    "\n",
    "val  localhour = udf( (ts: Timestamp, tz: String) => {\n",
    "  val dt = new DateTime(ts,DateTimeZone.forID(tz))\n",
    "  dt.getHourOfDay()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, our newly defined functions `dayofweek` and `localhour` can be mixed and matched with other Spark SQL functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- vid: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "val newyork_tz = \"America/New_York\"\n",
    "\n",
    "val df = df_ny.\n",
    "  withColumn(\"dow\",  dayofweek($\"ts\", lit(newyork_tz))).\n",
    "  withColumn(\"hour\", localhour($\"ts\", lit(newyork_tz))).\n",
    "  as(\"events\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---+----+\n",
      "|ts                   |dow|hour|\n",
      "+---------------------+---+----+\n",
      "|2010-04-18 21:23:48.0|6  |17  |\n",
      "|2010-04-18 22:33:48.0|6  |18  |\n",
      "|2010-04-18 23:38:56.0|6  |19  |\n",
      "|2010-04-19 12:04:48.0|0  |8   |\n",
      "|2010-04-19 13:53:21.0|0  |9   |\n",
      "+---------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// select time and the extracted day of the week and day of the month\n",
    "df.select(\"ts\", \"dow\", \"hour\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting user's preferences\n",
    "\n",
    "#### Basic statistics in Spark\n",
    "\n",
    "The following code section shows how to collect global statistics and histograms per hour of the day and per day of the week. Histograms can be made more specific by aggregating the events according to a number of factors, such as:\n",
    "\n",
    " - venue\n",
    " - geographical area\n",
    " - popular users\n",
    " - 1st, 2nd friend's circle\n",
    " \n",
    "If you are interested, in multiple slicing and dicing option, Spark as a [cube function](https://spark.apache.org/docs/1.5.1/api/scala/index.html#org.apache.spark.sql.DataFrame) as well.  \n",
    "To start, let's compute the histogram, accumulating all events and aggregating by hour of the day and by day of the week.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dow|count|\n",
      "+---+-----+\n",
      "|  1|15385|\n",
      "|  6|15849|\n",
      "|  3|16179|\n",
      "|  5|18085|\n",
      "|  4|16557|\n",
      "|  2|15687|\n",
      "|  0|14640|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// histogram day of the week events\n",
    "df.groupBy($\"dow\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|12  |7257 |\n",
      "|22  |4747 |\n",
      "|1   |1272 |\n",
      "|13  |8142 |\n",
      "|16  |7519 |\n",
      "|6   |937  |\n",
      "|3   |554  |\n",
      "|20  |7167 |\n",
      "|5   |550  |\n",
      "|19  |8970 |\n",
      "|15  |7254 |\n",
      "|17  |7495 |\n",
      "|9   |4586 |\n",
      "|4   |323  |\n",
      "|8   |3615 |\n",
      "|23  |3390 |\n",
      "|7   |2157 |\n",
      "|10  |4903 |\n",
      "|21  |6127 |\n",
      "|11  |5575 |\n",
      "|14  |7688 |\n",
      "|2   |852  |\n",
      "|0   |2681 |\n",
      "|18  |8621 |\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// histogram hour of the day events\n",
    "df.groupBy($\"hour\").count().show(24,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics in Spark: venue-specific histograms\n",
    "\n",
    "Here above we have extracted general patterns and looked at the histograms of events across all users and all venues. Moving on, let's have a look on how to create an specific histogram for each venue. \n",
    "\n",
    "#### Working with Vectors as DataFrame elements\n",
    "Working with vectors as DataFrame elements is a very powerful modeling technique, which by the way is extensively used in Spark ML. Having vectors as elements, allows you to use many vector arithmetic and logical operations as well as a great list of vector transformation as provided in the Spark ML library: http://spark.apache.org/docs/latest/ml-features.html, while you still keep the data in a DataFrame. \n",
    "\n",
    "In this tutorial, we will store the day and month histogram as a vector. First, let's convert the day-of-the-week to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import breeze.linalg._\n",
    "import breeze.linalg.DenseVector\n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vector,Vectors}\n",
    "\n",
    "def r(x: Double, d:Int) = { \n",
    "    import scala.math.{pow, round}\n",
    "    val p = pow(10,d); round(x*p)/p \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// vector histogram\n",
    "\n",
    "def toVector(i: Int, length:Int) = {\n",
    "  DenseVector((0 to length-1).map(x => if (x == i) 1.0 else 0.0).toArray)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are monday to sundays, ( values go from 0 to 6)  \n",
    "As an example let's see which vector is produced from this event on Friday.  \n",
    "the value 4, turns in to a '1' on the 5th position in the day of the week vector: it is a Friday :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17417,2010-04-18 21:23:48.0,6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"vid\", \"ts\", \"dow\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17417,DenseVector(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select($\"vid\", $\"dow\").rdd.map(r => (r.getLong(0),toVector(r.getInt(1), 7))).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair RDDs: reduce by Key\n",
    "\n",
    "We will now `reduceByKey` those weekly and daily vectors by applying vectors arithmetics. In this way, we can collect the probability of an event happening at a specific day of the week for each venue in the dataset. This is a much more detailed analysis since different venues, such as restaurants, musea and train stations have different daily and weekly histogram patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dow_hist = df.\n",
    "  select($\"vid\", $\"dow\").\n",
    "  rdd.\n",
    "  map(r => (r.getLong(0),toVector(r.getInt(1), 7))).\n",
    "  reduceByKey(_ + _).\n",
    "  mapValues(x => Vectors.dense((x / sum(x)).toArray.map(r(_,2)))).\n",
    "  toDF(\"vid\", \"dow_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below, the aggregated histogram, for a number of venues, where histogram values have been normalized so that their sum is one (such as in a descrete propability mass function https://en.wikipedia.org/wiki/Probability_mass_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------+\n",
      "|vid   |dow_hist                       |\n",
      "+------+-------------------------------+\n",
      "|513544|[0.67,0.0,0.0,0.0,0.0,0.33,0.0]|\n",
      "|178464|[0.0,0.0,0.0,0.0,1.0,0.0,0.0]  |\n",
      "|932648|[0.25,0.0,0.0,0.5,0.0,0.0,0.25]|\n",
      "+------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dow_hist.show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring events according to venues histograms\n",
    "\n",
    "Now we can use the day of the week histogram dataframe, to correlate when during the day a certain event occur. First let's join this dataframe with the dataframe of the events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+-------------+-------------+-----+---+---------------------------------+\n",
      "|ts                   |uid  |round(lat, 4)|round(lon, 4)|vid  |dow|dow_hist                         |\n",
      "+---------------------+-----+-------------+-------------+-----+---+---------------------------------+\n",
      "|2010-07-17 23:36:46.0|4750 |40.712       |-74.0096     |11745|5  |[0.1,0.06,0.1,0.06,0.16,0.42,0.1]|\n",
      "|2010-01-30 16:08:43.0|36070|40.712       |-74.0096     |11745|5  |[0.1,0.06,0.1,0.06,0.16,0.42,0.1]|\n",
      "|2010-03-13 14:25:18.0|49911|40.712       |-74.0096     |11745|5  |[0.1,0.06,0.1,0.06,0.16,0.42,0.1]|\n",
      "|2010-07-08 19:38:12.0|71895|40.712       |-74.0096     |11745|3  |[0.1,0.06,0.1,0.06,0.16,0.42,0.1]|\n",
      "|2010-02-13 22:51:17.0|22   |40.712       |-74.0096     |11745|5  |[0.1,0.06,0.1,0.06,0.16,0.42,0.1]|\n",
      "+---------------------+-----+-------------+-------------+-----+---+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.round\n",
    "\n",
    "val df_probs = df.\n",
    "  join(dow_hist, df(\"vid\") === dow_hist(\"vid\"), \"inner\").\n",
    "  select($\"ts\", $\"uid\", round($\"lat\",4), round($\"lon\",4), $\"events.vid\", $\"dow\", $\"dow_hist\")\n",
    "\n",
    "df_probs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profiling of users according to the visiting trend of venues\n",
    "\n",
    "Now we have sufficient ingredients to profile user behaviour, relative to the venue statistics. For instance, we can determine if a certain user prefer to go to a venue during peak hours of if he/she prefers to go there when it's quite. Everyone of us has certain rythms, habits and preferentces during the day and this analysis correlates the personal preferences with the venue histogram event distribution.\n",
    "\n",
    "As an example, we are going to calculate if a given user prefer to visit a given venue during peak hours, or rather off-peak when it's quiter. To do so we are going to calculate how \"trendy\" is a place during a given day of the week and add this \"feature\" to our events table. Once we have done it, we can start scoring user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+-----+---+---------+\n",
      "|ts                   |uid  |vid  |dow|dow_trend|\n",
      "+---------------------+-----+-----+---+---------+\n",
      "|2010-07-17 23:36:46.0|4750 |11745|5  |2.94     |\n",
      "|2010-01-30 16:08:43.0|36070|11745|5  |2.94     |\n",
      "|2010-03-13 14:25:18.0|49911|11745|5  |2.94     |\n",
      "+---------------------+-----+-----+---+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val  nth = udf( (i:Int, arr: Vector) => {\n",
    "  val v = arr.toArray.lift(i).getOrElse(0.0) \n",
    "  // more or less than average?\n",
    "  v * arr.toArray.length\n",
    "})\n",
    "\n",
    "df_probs.select($\"ts\", $\"uid\", $\"vid\", $\"dow\", round(nth($\"dow\", $\"dow_hist\"),2).as(\"dow_trend\")).show(3,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trendy places during a day of the week and hour of the day: putting it all together\n",
    "\n",
    "Let's repeat the same exercise for the histograms binned by hour of the day. And finally, let's merge and compute the probability of each given event, given the venue, the hour of the day, and the day of the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|vid   |hour_hist                                                                                           |\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|513544|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.33,0.33,0.0,0.0,0.0]|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// same for hour of the day\n",
    "\n",
    "val hour_hist = df.\n",
    "  select($\"vid\", $\"hour\").\n",
    "  rdd.\n",
    "  map(r => (r.getLong(0),toVector(r.getInt(1), 24))).\n",
    "  reduceByKey(_ + _).\n",
    "  mapValues(x => Vectors.dense((x / sum(x)).toArray.map(r(_,2)))).\n",
    "  toDF(\"vid\", \"hour_hist\")\n",
    "\n",
    "hour_hist.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+-------------+-------------+-----+----------+---------+\n",
      "|ts                   |uid  |round(lat, 4)|round(lon, 4)|vid  |hour_trend|dow_trend|\n",
      "+---------------------+-----+-------------+-------------+-----+----------+---------+\n",
      "|2010-07-17 23:36:46.0|4750 |40.712       |-74.0096     |11745|2.4       |2.94     |\n",
      "|2010-01-30 16:08:43.0|36070|40.712       |-74.0096     |11745|3.12      |2.94     |\n",
      "|2010-03-13 14:25:18.0|49911|40.712       |-74.0096     |11745|1.44      |2.94     |\n",
      "|2010-07-08 19:38:12.0|71895|40.712       |-74.0096     |11745|5.52      |0.42     |\n",
      "|2010-02-13 22:51:17.0|22   |40.712       |-74.0096     |11745|1.44      |2.94     |\n",
      "+---------------------+-----+-------------+-------------+-----+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_probs = df.\n",
    "  join(dow_hist, df(\"vid\") === dow_hist(\"vid\"), \"inner\").\n",
    "  join(hour_hist, df(\"vid\") === hour_hist(\"vid\"), \"inner\").\n",
    "  select( \n",
    "    $\"ts\", \n",
    "    $\"uid\", \n",
    "    round($\"lat\",4), \n",
    "    round($\"lon\",4),\n",
    "    $\"events.vid\",\n",
    "    round(nth($\"hour\", $\"hour_hist\"),2).as(\"hour_trend\"), \n",
    "    round(nth($\"dow\",  $\"dow_hist\"),2).as(\"dow_trend\"))\n",
    "\n",
    "df_probs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "Let's have a look at the most visited places for user 22.  \n",
    "This can be easily down by filtering the `df_probs` for uid=22 and then group by venue and picking the top 5 venues. In code here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  vid|count|\n",
      "+-----+-----+\n",
      "|12821|   40|\n",
      "|12579|   13|\n",
      "|23261|   13|\n",
      "|12505|   11|\n",
      "|12506|    8|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val uid = 22\n",
    "\n",
    "val df_userpref = df_probs.filter($\"uid\" === uid)\n",
    "val df_user_topvenues = df_userpref.groupBy(\"vid\").count().sort($\"count\".desc)\n",
    "df_user_topvenues.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+--------------------+---+-------------+-------------+------+----------+---------+\n",
      "|                  ts|uid|round(lat, 4)|round(lon, 4)|   vid|hour_trend|dow_trend|\n",
      "+--------------------+---+-------------+-------------+------+----------+---------+\n",
      "|2010-02-13 22:51:...| 22|       40.712|     -74.0096| 11745|      1.44|     2.94|\n",
      "|2009-11-11 22:16:...| 22|      40.7292|     -73.9814| 55671|      2.64|     0.77|\n",
      "|2010-02-18 15:03:...| 22|      40.7475|     -73.9787| 62327|      3.12|     1.75|\n",
      "|2009-11-17 20:51:...| 22|      40.7224|     -73.9887| 83615|      7.92|     2.31|\n",
      "|2010-02-13 23:04:...| 22|      40.7161|     -74.0042|101552|       6.0|     1.75|\n",
      "+--------------------+---+-------------+-------------+------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_userpref.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val topvenue = df_user_topvenues.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12821"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top_vid    = topvenue(0)\n",
    "top_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top_vcount = topvenue(1)\n",
    "top_vcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.7657052487"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top_vname  = venues.where($\"vid\" === top_vid).first()(1)\n",
    "top_vname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 22 visited vid: 12821 (40.7657052487), 40 times\n"
     ]
    }
   ],
   "source": [
    "println(s\"User $uid visited vid: $top_vid ($top_vname), ${top_vcount} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets analysing visiting patterns for user 22 on the top venue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 22 visited vid: 12821 (40.7657052487), 40 times\n"
     ]
    }
   ],
   "source": [
    "val topvenue = df_user_topvenues.first()\n",
    "\n",
    "val top_vid    = topvenue(0)\n",
    "val top_vcount = topvenue(1)\n",
    "val top_vname  = venues.where($\"vid\" === top_vid).first()(1)\n",
    "\n",
    "println(s\"User $uid visited vid: $top_vid ($top_vname), ${top_vcount} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting user preferences about this user in this place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_userpref.filter($\"vid\" === top_vid).select(\"dow_trend\", \"hour_trend\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks / Considerations\n",
    "\n",
    "From the statistics above seems that user 22, usually goes to his/her most favorite venue (Manhattan Park) when it's usually more crowded than usual. Definitely a social outgoing person :) . It seems therefore that this user seldom goes to the park when it's less crowded. \n",
    "\n",
    "Where to go from here? You could explore more features and statistics and start building a predictive model for each user on when and where a person might register an event. Looking at personal, venues, and other factors you could also establish if a given event is special or anomalous provided user preferences and habits and other venue statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you could also go further in the analysis, taking into consideration the relationships between users (for instance the friend user graph) or the relations between venues, either by looking at the sequence of the events of by looking at their geo-spacial proximity, by creating a density clustering on the venues. Some of these ideas are explained in a companion notebook \"Geo-located Clustering and Sequence Mining with Spark, Cassandra and Scala\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
